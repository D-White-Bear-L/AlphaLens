"""æµ‹è¯• WebScraper å·¥å…·ï¼ŒéªŒè¯ Windows äº‹ä»¶å¾ªç¯é—®é¢˜æ˜¯å¦å·²ä¿®å¤"""
import asyncio
import os
import sys
from pathlib import Path

from dotenv import load_dotenv
from loguru import logger

# åŠ è½½ç¯å¢ƒå˜é‡
env_paths = [
    Path(__file__).parent / ".env",
    Path(__file__).parent.parent / ".env",
    Path(__file__).parent.parent.parent / ".env",
]

for env_path in env_paths:
    if env_path.exists():
        load_dotenv(env_path)
        logger.info(f"Loaded .env from: {env_path}")
        break
else:
    load_dotenv()
    logger.info("Using default .env loading")

from app.tools.web_scraper import WebScraper, PLAYWRIGHT_AVAILABLE, USE_SYNC_API


def test_web_scraper_sync():
    """æµ‹è¯•åŒæ­¥è°ƒç”¨ WebScraperï¼ˆä¸åœ¨äº‹ä»¶å¾ªç¯ä¸­ï¼‰"""
    logger.info("=" * 60)
    logger.info("æµ‹è¯• 1: åŒæ­¥è°ƒç”¨ WebScraperï¼ˆä¸åœ¨äº‹ä»¶å¾ªç¯ä¸­ï¼‰")
    logger.info("=" * 60)
    
    if not PLAYWRIGHT_AVAILABLE:
        logger.warning("âš ï¸  Playwright æœªå®‰è£…ï¼Œè·³è¿‡æµ‹è¯•")
        return False
    
    logger.info(f"ä½¿ç”¨ {'åŒæ­¥' if USE_SYNC_API else 'å¼‚æ­¥'} API")
    
    try:
        scraper = WebScraper(
            url="https://www.example.com",
            extract_content=True
        )
        result = scraper()
        
        if result.get('success'):
            logger.info("âœ… æµ‹è¯•é€šè¿‡ï¼")
            logger.info(f"   æ ‡é¢˜: {result.get('title', 'N/A')}")
            logger.info(f"   å†…å®¹é•¿åº¦: {result.get('content_length', 0)} å­—ç¬¦")
            logger.info(f"   URL: {result.get('url', 'N/A')}")
            
            # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
            content = result.get('content', '')
            if content:
                preview_length = 300
                content_preview = content[:preview_length]
                if len(content) > preview_length:
                    content_preview += "..."
                logger.info(f"   å†…å®¹é¢„è§ˆ ({len(content)} å­—ç¬¦):")
                logger.info(f"   {'-' * 50}")
                content_lines = content.split('\n')
                for line in content_preview.split('\n')[:10]:  # æœ€å¤šæ˜¾ç¤º10è¡Œ
                    if line.strip():
                        logger.info(f"   {line[:80]}")
                if len(content_lines) > 10:
                    remaining_lines = len(content_lines) - 10
                    logger.info(f"   ... (è¿˜æœ‰ {remaining_lines} è¡Œ)")
                logger.info(f"   {'-' * 50}")
            
            return True
        else:
            logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {result.get('error', 'Unknown error')}")
            return False
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return False


async def test_web_scraper_async():
    """æµ‹è¯•åœ¨äº‹ä»¶å¾ªç¯ä¸­è°ƒç”¨ WebScraper"""
    logger.info("\n" + "=" * 60)
    logger.info("æµ‹è¯• 2: åœ¨äº‹ä»¶å¾ªç¯ä¸­è°ƒç”¨ WebScraper")
    logger.info("=" * 60)
    
    if not PLAYWRIGHT_AVAILABLE:
        logger.warning("âš ï¸  Playwright æœªå®‰è£…ï¼Œè·³è¿‡æµ‹è¯•")
        return False
    
    logger.info(f"ä½¿ç”¨ {'åŒæ­¥' if USE_SYNC_API else 'å¼‚æ­¥'} API")
    logger.info("æ³¨æ„: è¿™ä¸ªæµ‹è¯•ä¼šæ¨¡æ‹Ÿåœ¨å·²æœ‰äº‹ä»¶å¾ªç¯ä¸­è°ƒç”¨çš„æƒ…å†µ")
    
    try:
        scraper = WebScraper(
            url="https://www.example.com",
            extract_content=True
        )
        
        # åœ¨äº‹ä»¶å¾ªç¯ä¸­è°ƒç”¨ï¼ˆè¿™ä¼šè§¦å‘çº¿ç¨‹éš”ç¦»é€»è¾‘ï¼‰
        result = scraper()
        
        if result.get('success'):
            logger.info("âœ… æµ‹è¯•é€šè¿‡ï¼")
            logger.info(f"   æ ‡é¢˜: {result.get('title', 'N/A')}")
            logger.info(f"   å†…å®¹é•¿åº¦: {result.get('content_length', 0)} å­—ç¬¦")
            logger.info(f"   URL: {result.get('url', 'N/A')}")
            
            # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
            content = result.get('content', '')
            if content:
                preview_length = 300
                content_preview = content[:preview_length]
                if len(content) > preview_length:
                    content_preview += "..."
                logger.info(f"   å†…å®¹é¢„è§ˆ ({len(content)} å­—ç¬¦):")
                logger.info(f"   {'-' * 50}")
                content_lines = content.split('\n')
                for line in content_preview.split('\n')[:10]:  # æœ€å¤šæ˜¾ç¤º10è¡Œ
                    if line.strip():
                        logger.info(f"   {line[:80]}")
                if len(content_lines) > 10:
                    remaining_lines = len(content_lines) - 10
                    logger.info(f"   ... (è¿˜æœ‰ {remaining_lines} è¡Œ)")
                logger.info(f"   {'-' * 50}")
            
            return True
        else:
            logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {result.get('error', 'Unknown error')}")
            return False
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return False


async def test_web_scraper_multiple():
    """æµ‹è¯•å¤šæ¬¡è°ƒç”¨ WebScraperï¼ˆæ¨¡æ‹Ÿå®é™…ä½¿ç”¨åœºæ™¯ï¼‰"""
    logger.info("\n" + "=" * 60)
    logger.info("æµ‹è¯• 3: å¤šæ¬¡è°ƒç”¨ WebScraperï¼ˆæ¨¡æ‹Ÿ Agent ä½¿ç”¨åœºæ™¯ï¼‰")
    logger.info("=" * 60)
    
    if not PLAYWRIGHT_AVAILABLE:
        logger.warning("âš ï¸  Playwright æœªå®‰è£…ï¼Œè·³è¿‡æµ‹è¯•")
        return False
    
    test_urls = [
        "https://www.example.com",
        "https://httpbin.org/html",
        "https://www.example.com"  # é‡å¤æµ‹è¯•
    ]
    
    success_count = 0
    for i, url in enumerate(test_urls, 1):
        logger.info(f"\næµ‹è¯• URL {i}/{len(test_urls)}: {url}")
        try:
            scraper = WebScraper(url=url, extract_content=True)
            result = scraper()
            
            if result.get('success'):
                logger.info(f"  âœ… æˆåŠŸ: {result.get('title', 'N/A')[:50]}")
                content = result.get('content', '')
                if content:
                    preview = content[:150].replace('\n', ' ')
                    if len(content) > 150:
                        preview += "..."
                    logger.info(f"     å†…å®¹é¢„è§ˆ: {preview}")
                success_count += 1
            else:
                logger.warning(f"  âš ï¸  å¤±è´¥: {result.get('error', 'Unknown')}")
        except Exception as e:
            logger.error(f"  âŒ å¼‚å¸¸: {str(e)}")
    
    logger.info(f"\nç»“æœ: {success_count}/{len(test_urls)} æˆåŠŸ")
    return success_count > 0


async def test_web_scraper_real_urls():
    """æµ‹è¯•çœŸå® URLï¼ˆå¯èƒ½ä¼šå¤±è´¥çš„ï¼‰"""
    logger.info("\n" + "=" * 60)
    logger.info("æµ‹è¯• 4: æµ‹è¯•çœŸå® URLï¼ˆåŒ…æ‹¬å¯èƒ½å¤±è´¥çš„ï¼‰")
    logger.info("=" * 60)
    
    if not PLAYWRIGHT_AVAILABLE:
        logger.warning("âš ï¸  Playwright æœªå®‰è£…ï¼Œè·³è¿‡æµ‹è¯•")
        return False
    
    test_urls = [
        {
            "url": "https://www.example.com",
            "expected": True,
            "description": "ç®€å•æµ‹è¯•é¡µé¢"
        },
        {
            "url": "https://zh.wikipedia.org/zh-hans/%E8%94%A1%E5%BE%90%E5%9D%A4%E7%AF%AE%E7%90%83%E8%A7%86%E9%A2%91%E4%BA%8B%E4%BB%B6",
            "expected": True,
            "description": "HTML æµ‹è¯•é¡µé¢"
        },
        {
            "url": "https://www.baidu.com",
            "expected": True,
            "description": "ç™¾åº¦é¦–é¡µï¼ˆå¯èƒ½è¾ƒæ…¢ï¼‰"
        }
    ]
    
    results = []
    for test_case in test_urls:
        url = test_case["url"]
        description = test_case["description"]
        logger.info(f"\næµ‹è¯•: {description}")
        logger.info(f"URL: {url}")
        
        try:
            scraper = WebScraper(url=url, extract_content=True)
            result = scraper()
            
            if result.get('success'):
                logger.info(f"  âœ… æˆåŠŸ")
                logger.info(f"     æ ‡é¢˜: {result.get('title', 'N/A')[:60]}")
                logger.info(f"     å†…å®¹é•¿åº¦: {result.get('content_length', 0)} å­—ç¬¦")
                
                # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
                content = result.get('content', '')
                if content:
                    preview_length = 400
                    content_preview = content[:preview_length]
                    if len(content) > preview_length:
                        content_preview += "..."
                    logger.info(f"     å†…å®¹é¢„è§ˆ:")
                    logger.info(f"     {'-' * 60}")
                    content_lines = content.split('\n')
                    for line in content_preview.split('\n')[:15]:  # æœ€å¤šæ˜¾ç¤º15è¡Œ
                        if line.strip():
                            logger.info(f"     {line[:70]}")
                    if len(content_lines) > 15:
                        remaining_lines = len(content_lines) - 15
                        logger.info(f"     ... (è¿˜æœ‰ {remaining_lines} è¡Œ)")
                    logger.info(f"     {'-' * 60}")
                
                results.append(True)
            else:
                logger.warning(f"  âš ï¸  å¤±è´¥: {result.get('error', 'Unknown')}")
                results.append(False)
        except Exception as e:
            logger.error(f"  âŒ å¼‚å¸¸: {str(e)}")
            results.append(False)
    
    success_count = sum(results)
    logger.info(f"\nç»“æœ: {success_count}/{len(test_urls)} æˆåŠŸ")
    return success_count > 0


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("å¼€å§‹æµ‹è¯• WebScraper å·¥å…·...")
    logger.info(f"å·¥ä½œç›®å½•: {os.getcwd()}")
    logger.info(f"Playwright å¯ç”¨: {PLAYWRIGHT_AVAILABLE}")
    if PLAYWRIGHT_AVAILABLE:
        logger.info(f"ä½¿ç”¨ API ç±»å‹: {'åŒæ­¥' if USE_SYNC_API else 'å¼‚æ­¥'}")
    
    if not PLAYWRIGHT_AVAILABLE:
        logger.error("âŒ Playwright æœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œæµ‹è¯•")
        logger.info("è¯·è¿è¡Œ: pip install playwright")
        logger.info("ç„¶åè¿è¡Œ: playwright install chromium")
        return
    
    # æµ‹è¯• 1: åŒæ­¥è°ƒç”¨
    test1_result = test_web_scraper_sync()
    
    # æµ‹è¯• 2: åœ¨äº‹ä»¶å¾ªç¯ä¸­è°ƒç”¨
    test2_result = await test_web_scraper_async()
    
    # æµ‹è¯• 3: å¤šæ¬¡è°ƒç”¨
    test3_result = await test_web_scraper_multiple()
    
    # æµ‹è¯• 4: çœŸå® URL
    test4_result = await test_web_scraper_real_urls()
    
    # æ€»ç»“
    logger.info("\n" + "=" * 60)
    logger.info("æµ‹è¯•æ€»ç»“")
    logger.info("=" * 60)
    logger.info(f"æµ‹è¯• 1 (åŒæ­¥è°ƒç”¨): {'âœ… é€šè¿‡' if test1_result else 'âŒ å¤±è´¥'}")
    logger.info(f"æµ‹è¯• 2 (äº‹ä»¶å¾ªç¯ä¸­): {'âœ… é€šè¿‡' if test2_result else 'âŒ å¤±è´¥'}")
    logger.info(f"æµ‹è¯• 3 (å¤šæ¬¡è°ƒç”¨): {'âœ… é€šè¿‡' if test3_result else 'âŒ å¤±è´¥'}")
    logger.info(f"æµ‹è¯• 4 (çœŸå® URL): {'âœ… é€šè¿‡' if test4_result else 'âŒ å¤±è´¥'}")
    
    all_passed = all([test1_result, test2_result, test3_result, test4_result])
    if all_passed:
        logger.info("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    else:
        logger.warning("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
    
    logger.info("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())

